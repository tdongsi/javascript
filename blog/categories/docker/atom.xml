<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Docker | Archived Blog Posts]]></title>
  <link href="http://tdongsi.github.io/javascript/blog/categories/docker/atom.xml" rel="self"/>
  <link href="http://tdongsi.github.io/javascript/"/>
  <updated>2021-03-21T05:45:14-07:00</updated>
  <id>http://tdongsi.github.io/javascript/</id>
  <author>
    <name><![CDATA[Cuong Dong-Si]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Docker: Copy File Into a Container]]></title>
    <link href="http://tdongsi.github.io/javascript/blog/2017/02/09/docker-copy-file-into-a-container/"/>
    <updated>2017-02-09T15:17:19-08:00</updated>
    <id>http://tdongsi.github.io/javascript/blog/2017/02/09/docker-copy-file-into-a-container</id>
    <content type="html"><![CDATA[<p>In this blog post, we have a running Docker container or a running pod in Kubernetes cluster.
We want to add some files into the running containers to fix some issue, verify, and commit the changes.</p>

<!--more-->


<h3>Best-case scenario: <code>docker cp</code></h3>

<p>The most obvious way is to create a Dockerfile and rebuild the Docker image.
The Dockerfile will look like this:</p>

<pre><code class="plain Dockerfile">FROM olderImage
ADD myfile /path/myfile
...
</code></pre>

<p>However, in this approach, we need to stop the Docker containers, update, and re-run with the new Docker images.
It does not work if we want to work with <strong><em>running</em></strong> containers.
For running containers, the better way to add files into containers is to copy files into containers.
For the more updated versions of Docker (1.8+), the recommended way for copying is to use <a href="https://docs.docker.com/engine/reference/commandline/cp/"><code>docker cp</code> command</a>.</p>

<h3>Copy file directly</h3>

<p><code>docker cp</code> does not always work, especially in older versions of Docker.
In older versions of Docker, the <code>docker cp</code> command only allowed copying files from a <strong>container</strong> to the <strong>host</strong>.
Only since Docker 1.8, copying files from the host to a container is added.
You will get some error with unhelpful messages like this in older versions of Docker:</p>

<pre><code class="plain Unsupported "docker cp"">[centos@comp ~]$ ls maven_3.3.9-3_all.deb
maven_3.3.9-3_all.deb

[centos@comp ~]$ sudo docker cp maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins
FATA[0000] Error: Path not specified
[centos@comp ~]$ sudo docker cp ./maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins
FATA[0000] Error: Path not specified
[centos@comp ~]$ sudo docker cp ./maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins/
FATA[0000] Error: Path not specified
[centos@comp ~]$ sudo docker cp maven_3.3.9-3_all.deb 9a8d782156ca:/home/jenkins/maven_3.3.9-3_all.deb
FATA[0000] Error: Path not specified
</code></pre>

<p>If you find yourself stuck with older versions of Docker, the alternative is to manually copy the files from hosts filesystem to containers filesystem location.
First, you need to determine where the containers filesystem (volume) is mounted on the host:</p>

<pre><code class="plain Using inspect to find Volume location">[centos@comp ~]$ sudo docker ps
CONTAINER ID      IMAGE    COMMAND ...
9a8d782156ca

[centos@comp ~]$ sudo docker inspect -f { {.Id} } 9a8d782156ca
9a8d782156ca9a3bd59545a18943de408ca58f42c4389c12e9bb43f4ad239d52

[centos@comp ~]$ sudo docker inspect -f { {.Volumes} } 9a8d782156ca
map[/home/jenkins:/var/lib/docker/vfs/dir/b051cc2b086c53ce436ad82b9332ba79687f3ddcf8ee77e3f8264e7cafe32438]
[centos@comp ~]$ sudo ls /var/lib/docker/vfs/dir/b051cc2b086c53ce436ad82b9332ba79687f3ddcf8ee77e3f8264e7cafe32438
test.txt
</code></pre>

<p>NOTE: In the shell commands above, there is no space between <code>{</code> (space is added for Jekyll blog engine).
After the mounting path is determined, you can manipulate the container'ss filesystem directly, including copying files into it.</p>

<pre><code class="plain Directly copy file into containers filesystem">[centos@comp ~]$ sudo cp maven_3.3.9-3_all.deb /var/lib/docker/vfs/dir/b051cc2b086c53ce436ad82b9332ba79687f3ddcf8ee77e3f8264e7cafe32438
</code></pre>

<p>You can verify such manipulation by <code>docker exec</code>-ing into the container and verify the files:</p>

<pre><code class="plain Before and After">jenkins@9a8d782156ca:~$ ls
test.txt

jenkins@9a8d782156ca:~$ ls
maven_3.3.9-3_all.deb  test.txt
</code></pre>

<h3>Reference</h3>

<ul>
<li><a href="https://docs.docker.com/engine/reference/commandline/cp/">docker cp</a></li>
<li><a href="http://stackoverflow.com/questions/22907231/copying-files-from-host-to-docker-container">Stackoverflow discussion</a></li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Kubernetes: Pod-to-Node Communication Loss]]></title>
    <link href="http://tdongsi.github.io/javascript/blog/2017/01/24/kubernetes-pod-to-node-communication-loss/"/>
    <updated>2017-01-24T15:05:15-08:00</updated>
    <id>http://tdongsi.github.io/javascript/blog/2017/01/24/kubernetes-pod-to-node-communication-loss</id>
    <content type="html"><![CDATA[<p>This post goes over what happens if we misconfigure <code>etcd</code> and <code>flannel</code> to use the same network (e.g., &ldquo;10.252.61.0/16&rdquo;) as the infrastructure (e.g., &ldquo;10.252.158.72&rdquo; node).
This newbie mistake is rare but very perplexing and this post shows how to troubleshoot it with <code>busybox</code> container.</p>

<!--more-->


<h3>Problem symptoms</h3>

<p>From a pod (e.g., <code>jenkins</code>) on one node (e.g., <code>10.252.158.71</code>), we cannot communicate with another node (e.g., <code>10.252.158.72</code>) even though two nodes can communicate with each other normally.</p>

<pre><code class="plain">mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig exec -it jenkins -- bash -il
jenkins@jenkins:~$ ping 10.252.158.72
PING 10.252.158.72 (10.252.158.72) 56(84) bytes of data.
^C
--- 10.252.158.72 ping statistics ---
16 packets transmitted, 0 received, 100% packet loss, time 14999ms

jenkins@jenkins:~$ exit
</code></pre>

<p>Even more perplexing, the pod-to-pod communication is fine (as described right below), even though the second pod is on the same node (e.g., <code>10.252.158.72</code>) that the first pod cannot communciate to.</p>

<h3>Troubleshooting with <code>busybox</code></h3>

<p>Try to run a test pod <code>busybox</code>.
<code>jenkins</code> pod can ping the <code>busybox</code> pod, but not the node that <code>busybox</code> pod is running on.</p>

<pre><code>mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig run busybox \
--image=docker.registry.company.net/tdongsi/busybox --restart=Never --tty -i --generator=run-pod/v1
Waiting for pod default/busybox to be running, status is Pending, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false

mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig exec -it jenkins -- bash -il
jenkins@jenkins:~$ ping 10.252.61.7
PING 10.252.61.7 (10.252.61.7) 56(84) bytes of data.
64 bytes from 10.252.61.7: icmp_seq=1 ttl=62 time=0.540 ms
64 bytes from 10.252.61.7: icmp_seq=2 ttl=62 time=0.186 ms
64 bytes from 10.252.61.7: icmp_seq=3 ttl=62 time=0.177 ms
64 bytes from 10.252.61.7: icmp_seq=4 ttl=62 time=0.161 ms
64 bytes from 10.252.61.7: icmp_seq=5 ttl=62 time=0.187 ms
^C
--- 10.252.61.7 ping statistics ---
5 packets transmitted, 5 received, 0% packet loss, time 4000ms
rtt min/avg/max/mdev = 0.161/0.250/0.540/0.145 ms

jenkins@jenkins:~$ ping 10.252.158.72
PING 10.252.158.72 (10.252.158.72) 56(84) bytes of data.
^C
--- 10.252.158.72 ping statistics ---
14 packets transmitted, 0 received, 100% packet loss, time 13000ms
</code></pre>

<p>In this case, we would use <code>traceroute</code> from the <code>busybox</code> container to determine when the packets are dropped.
<code>10.252.158.72</code> is IP of the VM. <code>10.252.100.5</code> is the IP of the <code>jenkins</code> pod.</p>

<pre><code>mymac:kubernetes tdongsi$ kubectl --kubeconfig kubeconfig run busybox \
--image=docker.registry.company.net/tdongsi/busybox --restart=Never --tty -i --generator=run-pod/v1

Waiting for pod default/busybox to be running, status is Pending, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false
Waiting for pod default/busybox to be running, status is Running, pod ready: false

/ # traceroute 10.252.158.72
traceroute to 10.252.158.72 (10.252.158.72), 30 hops max, 46 byte packets
 1  10.252.61.1 (10.252.61.1)  0.005 ms  0.012 ms  0.001 ms
 2  *  *  *
 3  *  *  *
 4  *  *  *
 5  *  *  *
/ #
/ # traceroute 10.252.100.5
traceroute to 10.252.100.5 (10.252.100.5), 30 hops max, 46 byte packets
 1  10.252.61.1 (10.252.61.1)  0.005 ms  0.004 ms  0.002 ms
 2  *  10.252.100.0 (10.252.100.0)  0.487 ms  0.241 ms
 3  10.252.100.5 (10.252.100.5)  0.141 ms  0.563 ms  0.132 ms
/ # exit
</code></pre>

<p>For the context, <code>10.252.100.5</code> is the IP of the service, as shown in the command below.</p>

<pre><code>mymac:private_cloud tdongsi$ kubectl --kubeconfig kubeconfig describe services
Name:           jenkins
Namespace:      default
Labels:         &lt;none&gt;
Selector:       name=jenkins
Type:           NodePort
IP:         10.252.77.85
Port:           http    80/TCP
NodePort:       http    30080/TCP
Endpoints:      10.252.100.5:8080
Session Affinity:   None
No events.
</code></pre>

<h3>What went wrong?</h3>

<p>It&rsquo;s a newbie mistake when configuring Kubernetes.
When setting up <code>etcd</code> and configuring it to hold <code>flannel</code> configuration, it is important to pick an unused network.
I made a mistake for using <code>10.252.61.0/16</code> for flannel when some of my kubernetes nodes has IPs as &ldquo;10.252.xxx.xxx&rdquo;.
As a result, kube-proxy services intercept the traffic from the container and thinks its a virtual traffic since my node IP happens to be in the same subnet with <code>flanneld</code>.
This leads to pod-to-VM communication loss as described above.
The solution is simply reset flanneld with another subnet after resetting configruation value in <code>etcd</code> to &ldquo;172.17.0.0/16&rdquo;.</p>

<pre><code class="plain Update etcd">[centos@kube-master ~]$ etcdctl update /kube-centos/network/config \
"{ \"Network\": \"172.17.0.0/16\", \"SubnetLen\": 24, \"Backend\": { \"Type\": \"vxlan\" } }"

[centos@kube-master ~]$ etcdctl rm --recursive /kube-centos/network/subnets
[centos@kube-master ~]$ etcdctl ls /kube-centos/network
/kube-centos/network/config
</code></pre>

<p>After this, we can reset and restart <code>flannel</code> services on all nodes to use the new network overlay configuration.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Docker Image for ETL Development in Vertica]]></title>
    <link href="http://tdongsi.github.io/javascript/blog/2016/09/01/docker-image-for-vertica/"/>
    <updated>2016-09-01T11:38:27-07:00</updated>
    <id>http://tdongsi.github.io/javascript/blog/2016/09/01/docker-image-for-vertica</id>
    <content type="html"><![CDATA[<p>Docker is Awesome!!!</p>

<!--more-->


<p>I wish I knew Docker earlier, before going through the hassle of creating VMs for local ETL development and testing.
Docker can make the whole setup even easier.
It can be done in just a few commands, using <a href="https://github.com/tdongsi/vertica/tree/master/docker">a Vertica Dockerfile</a>, created based on <a href="https://github.com/wmarinho/docker-hp-vertica">this</a>.
In addition to easy virtualization, Docker also enables the entire setup can be automated in a script, allowing it to be version-controlled (i.e., <a href="https://en.wikipedia.org/wiki/Infrastructure_as_Code">Infrastructure as Code</a>).</p>

<p>Some notes about this Dockerfile, compared to <code>wmarinho</code>&rsquo;s:</p>

<ul>
<li>Added new schema, new user and new role as examples. Avoid using <code>dbadmin</code> user for development purpose.</li>
<li>Added Java and Maven for Java-based ETL and automated test execution.</li>
<li>Demonstrated running Bash and SQL scripts to initialize the container/database.</li>
</ul>


<h3>How to run</h3>

<p>Before running <code>docker build</code>, download Vertica Community Edition from <a href="https://my.vertica.com/">https://my.vertica.com/</a> and place in the same folder as the <code>Dockerfile</code>.
This <code>Dockerfile</code> takes &ldquo;vertica-7.2.3-0.x86_64.RHEL6.rpm&rdquo; as the install file.</p>

<pre><code class="plain Windows output">epigineer@epigineerpc MINGW64 /c/Work/Github/vertica/docker (develop)
$ docker build -t vertica .
...

epigineer@epigineerpc MINGW64 /c/Work/Github/vertica/docker (develop)
$ docker images
REPOSITORY          TAG                 IMAGE ID            CREATED
SIZE
vertica             latest              d2607fa1f457        13 seconds ago
1.638 GB
&lt;none&gt;              &lt;none&gt;              486163abe73f        11 minutes ago
1.638 GB
centos              centos6.6           2c886f766286        8 weeks ago
202.6 MB

epigineer@epigineerpc MINGW64 /c/Work/Github/vertica/docker (develop)
$ docker run -p 5433:5433 --hostname=verthost --privileged=true --memory 4G -t
-i d2607fa1f457 /bin/bash
Info: no password specified, using none
        Starting nodes:
                v_docker_node0001 (127.0.0.1)
        Starting Vertica on all nodes. Please wait, databases with large catalog
 may take a while to initialize.
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (DOWN)
        Node Status: v_docker_node0001: (UP)
Database docker started successfully
creating schema
CREATE SCHEMA
creating user
CREATE USER
creating role
CREATE ROLE
grant usage, create on schema
GRANT PRIVILEGE
</code></pre>

<h3>Troubleshooting Notes</h3>

<p>In Mac OSX, remember that the <code>entrypoint.sh</code> file should have executable permission.
Otherwise, you might get the error &ldquo;oci runtime error: exec: &rdquo;/entrypoint.sh": permission denied".
After changing the file permission, you have to rebuild the image with <code>docker build</code> before <code>docker run</code> again.</p>

<h4>&ldquo;Insufficient resources&rdquo; error when running ETL</h4>

<p>You might get &ldquo;Insufficient resources to execute plan on pool general &hellip; Memory exceeded&rdquo; error when running a large ETL script against the Vertica container.
For complex ETL, Vertica might need additional memory to execute the query plan.
Simply setting higher memory allocation using <code>--memory</code> option of <code>docker run</code> might NOT work if using <strong>Docker Toolbox</strong>.
To set higher memory allowance, stop the <code>docker-machine</code> and set memory as follows:</p>

<pre><code class="plain">tdongsi$ docker-machine stop
Stopping "default"...
Machine "default" was stopped.

tdongsi$ VBoxManage modifyvm default --memory 8192

tdongsi$ docker-machine start
Starting "default"...
(default) Check network to re-create if needed...
(default) Waiting for an IP...
Machine "default" was started.
Waiting for SSH to be available...
Detecting the provisioner...
Started machines may have new IP addresses. You may need to re-run the `docker-machine env` command.
</code></pre>

<p>Note that after running the above commands, <code>docker-machine inspect</code> still shows <code>"Memory":"2048"</code>.
To verify if memory is properly allocated as desired, run <code>free</code> command, for example, inside the container to verify.</p>

<h3>Links</h3>

<ul>
<li><a href="https://github.com/tdongsi/vertica/tree/master/docker">My Dockerfile for ETL development and testing on Vertica</a></li>
<li><a href="https://github.com/wmarinho/docker-hp-vertica">Original Dockerfile</a></li>
<li><a href="https://www.docker.com/">Docker</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
